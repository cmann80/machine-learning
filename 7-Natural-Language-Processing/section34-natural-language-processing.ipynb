{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Restaurant_Reviews.tsv\", delimiter=\"\\t\", quoting = 3) \n",
    "# the independent and dependent variables in TSV files are delimited by tabs because commas (the c in CSV) are used in the text itself\n",
    "# the \"quoting = 3\" parameter instructs pandas to ignore all quotation marks \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# regex package\n",
    "import nltk\n",
    "# a package of NLP tools\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "# stopwords are words that do not benefit bag of words analysis like \"a\", \"the\" \"it\" etc.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# a class for getting rid of inflectional affixes so each stem gets just one entry in the sparse matrix\n",
    "corpus = []\n",
    "# will contain the cleaned dataset\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \", dataset[\"Review\"][i])\n",
    "    # replaces all non-alphabet characters with a space\n",
    "    review = review.lower()\n",
    "    # changes all to lower case\n",
    "    review = review.split()\n",
    "    # splits into individual words\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words(\"english\")\n",
    "    all_stopwords.remove(\"not\")\n",
    "    # The list of stopwords may contain words that can influence sentiment, so I manually removed them from the list\n",
    "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "    # only stem if the word is not in the list of English stopwords\n",
    "    review = \" \".join(review)\n",
    "    # join the review back together with spaces between words\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "# the total feature count is 1566. There's a sweet spot for the number of max features; it appears to be around 500 here\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, -1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "# Changed the test size to 25% so it's nice round numbers in a 400 entry test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Naive Bayes model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1 )), 1)) \n",
    "# predictions on the left, real data on the right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
