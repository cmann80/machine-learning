{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Ads_CTR_Optimisation.csv\")\n",
    "# CTR = click through rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Thompson Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N = 500     # number of rounds (running with all 10000 rounds is no better than just counting\n",
    "                # The strength of a reinforcement learning algorithm is the ability to see the most productive ad with many fewer rounds\n",
    "d = 10 # number of ads\n",
    "ad_selected = [] # will eventually be a list with 10,000 entries\n",
    "\n",
    "numbers_of_rewards_1 = [0] * d\n",
    "numbers_of_rewards_0 = [0] * d\n",
    "\n",
    "total_reward = 0 # accumulates all rewards\n",
    "\n",
    "for n in range(0, N):\n",
    "        ad = 0 # the index of the ad selected in each round\n",
    "        max_random = 0 # the maximum number of random draws an ad gets\n",
    "        for i in range (0, d):\n",
    "                random_beta = random.betavariate(numbers_of_rewards_1[i] + 1, numbers_of_rewards_0[i] + 1)\n",
    "                if random_beta > max_random:\n",
    "                        max_random = random_beta\n",
    "                        ad = i\n",
    "                # no else needed; it was needed in UCB because of its different math\n",
    "        ad_selected.append(ad)\n",
    "        reward = dataset.values[n, ad]\n",
    "        if reward == 1:\n",
    "                numbers_of_rewards_1[ad] += 1\n",
    "        else:\n",
    "                numbers_of_rewards_0[ad] += 1\n",
    "        total_reward = total_reward + reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ad_selected) #which ad was selected in each round\n",
    "plt.title(\"Histogram of ads selections\")\n",
    "plt.xlabel(\"Ads\")\n",
    "plt.ylabel(\"Number of times each ad was selected\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
